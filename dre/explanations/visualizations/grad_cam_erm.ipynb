{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import Restart\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "\n",
    "sys.path.append('../../../dre')\n",
    "from datasets import make_dataset\n",
    "from mixuploss import MixupLoss\n",
    "from networks import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_base = ResNet()\n",
    "net_base = nn.DataParallel(net_base)\n",
    "net_base.module.network.fc = nn.Linear(net_base.module.network.fc.in_features, 10)\n",
    "net_base.load_state_dict(torch.load('../../ckpts/baseline_model.pth'))\n",
    "net_base = net_base.to(device)\n",
    "net_base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ours = ResNet()\n",
    "net_ours = nn.DataParallel(net_ours)\n",
    "net_ours.module.network.fc = nn.Linear(net_ours.module.network.fc.in_features, 10)\n",
    "net_ours.load_state_dict(torch.load('../../ckpts/dre_model.pth'))\n",
    "net_ours = net_ours.to(device)\n",
    "net_ours.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            # transforms.Resize((224,224)),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),\n",
    "            transforms.RandomGrayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "transform_orig = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset38 = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_38/', transform=transform)\n",
    "dset43 = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_43/', transform=transform)\n",
    "dset46 = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_46/', transform=transform)\n",
    "dset100 = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_100/', transform=transform)\n",
    "\n",
    "dset38_orig = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_38/', transform=transform_orig)\n",
    "dset43_orig = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_43/', transform=transform_orig)\n",
    "dset46_orig = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_46/', transform=transform_orig)\n",
    "dset100_orig = torchvision.datasets.ImageFolder('../../../data/terra_incognita/location_100/', transform=transform_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bird', 'bobcat', 'cat', 'coyote', 'dog', 'empty', 'opossum',\n",
       "       'rabbit', 'raccoon', 'squirrel'], dtype='<U8')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = np.sort(dset38.classes)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "imgsz = 224\n",
    "\n",
    "loader38 = torch.utils.data.DataLoader(dset38, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader43 = torch.utils.data.DataLoader(dset43, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader46 = torch.utils.data.DataLoader(dset46, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader100 = torch.utils.data.DataLoader(dset100, batch_size=bs, shuffle=False, num_workers=8)\n",
    "\n",
    "loader38_orig = torch.utils.data.DataLoader(dset38_orig, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader43_orig = torch.utils.data.DataLoader(dset43_orig, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader46_orig = torch.utils.data.DataLoader(dset46_orig, batch_size=bs, shuffle=False, num_workers=8)\n",
    "loader100_orig = torch.utils.data.DataLoader(dset100_orig, batch_size=bs, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_gc_base = LayerGradCam(net_base, net_base.module.network.layer4)\n",
    "layer_gc_ours = LayerGradCam(net_ours, net_ours.module.network.layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr_scale(x):\n",
    "    return (x-x.min()) / (x.max()-x.min())\n",
    "\n",
    "def explain(image, image_orig, pred_base, pred_ours, count, imgsz=224, target_env='L38', mode='ood'):\n",
    "    image = image.reshape(1, 3, imgsz, imgsz).to(device)\n",
    "    image_orig = image_orig.reshape(1, 3, imgsz, imgsz).to(device)\n",
    "\n",
    "    attr_base = layer_gc_base.attribute(image, target=pred_base)\n",
    "    attr_ours = layer_gc_ours.attribute(image, target=pred_ours)\n",
    "\n",
    "    attribution_base = F.interpolate(attr_base, size=imgsz, mode='bilinear').squeeze()\n",
    "    attribution_ours = F.interpolate(attr_ours, size=imgsz, mode='bilinear').squeeze()\n",
    "\n",
    "    attribution_base = attr_scale(attribution_base)\n",
    "    attribution_ours = attr_scale(attribution_ours)\n",
    "\n",
    "    cmap = mpl.cm.get_cmap('jet', 256)\n",
    "    heatmap_base = cmap(attribution_base.cpu().detach().numpy(), alpha = 0.5)\n",
    "    heatmap_ours = cmap(attribution_ours.cpu().detach().numpy(), alpha = 0.5)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(4, 2), dpi=200)\n",
    "    # fig.suptitle('Grad-CAM', fontsize=20)\n",
    "\n",
    "    ax[0].imshow(image_orig.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[0].set_title(str(mode) + ' ' + str(target_env))\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(image_orig.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[1].set_title('baseline')\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(image_orig.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[2].set_title('ours')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    ax[1].imshow(heatmap_base)\n",
    "    ax[1].axis('off')\n",
    "    ax[2].imshow(heatmap_ours)\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    fig.savefig('./erm/{}/{}/{}/{}_{}.jpeg'.format(mode, target_env, str(class_names[int(pred_ours.cpu().detach().numpy())]), str(count), str(class_names[int(pred_ours.cpu().detach().numpy())])))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize single class\n",
    "\n",
    "count = 0\n",
    "it = iter(loader46)\n",
    "it_orig = iter(loader46_orig)\n",
    "\n",
    "for batch_idx in tqdm(range(len(it))):\n",
    "    inputs, targets = next(it)\n",
    "    inputs_orig, _ = next(it_orig)\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    outputs_base = net_base(inputs)\n",
    "    outputs_ours = net_ours(inputs)\n",
    "\n",
    "    preds_base = outputs_base.argmax(dim=1)\n",
    "    preds_ours = outputs_ours.argmax(dim=1)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        if targets[i] == 2:\n",
    "            if preds_ours[i] == targets[i]:\n",
    "                explain(inputs[i], inputs_orig[i], preds_base[i], preds_ours[i], count, target_env='L38', mode='ood')\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02ffe4940dce8a26db6b5238b5727e42acd2513365a3fdb623eeb2bd329ab7e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
